{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:36:47.154628Z\",\"iopub.execute_input\":\"2025-11-08T14:36:47.155133Z\",\"iopub.status.idle\":\"2025-11-08T14:36:47.197763Z\",\"shell.execute_reply.started\":\"2025-11-08T14:36:47.155017Z\",\"shell.execute_reply\":\"2025-11-08T14:36:47.196369Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:36:47.199961Z\",\"iopub.execute_input\":\"2025-11-08T14:36:47.200692Z\",\"iopub.status.idle\":\"2025-11-08T14:36:47.210212Z\",\"shell.execute_reply.started\":\"2025-11-08T14:36:47.200656Z\",\"shell.execute_reply\":\"2025-11-08T14:36:47.209356Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\nfrom sklearn.metrics import make_scorer, roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:36:47.211266Z\",\"iopub.execute_input\":\"2025-11-08T14:36:47.211526Z\",\"iopub.status.idle\":\"2025-11-08T14:36:51.215334Z\",\"shell.execute_reply.started\":\"2025-11-08T14:36:47.211505Z\",\"shell.execute_reply\":\"2025-11-08T14:36:51.214532Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndf_train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")\n\n# %% [code] {\"_kg_hide-output\":false,\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:36:51.217898Z\",\"iopub.execute_input\":\"2025-11-08T14:36:51.218305Z\",\"iopub.status.idle\":\"2025-11-08T14:36:51.242049Z\",\"shell.execute_reply.started\":\"2025-11-08T14:36:51.218282Z\",\"shell.execute_reply\":\"2025-11-08T14:36:51.241152Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndf_train.head(2)\n\n# %% [code] {\"_kg_hide-output\":true,\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:36:51.242980Z\",\"iopub.execute_input\":\"2025-11-08T14:36:51.243211Z\",\"iopub.status.idle\":\"2025-11-08T14:36:51.266380Z\",\"shell.execute_reply.started\":\"2025-11-08T14:36:51.243194Z\",\"shell.execute_reply\":\"2025-11-08T14:36:51.265409Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndf_test.head(2)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:36:51.267690Z\",\"iopub.execute_input\":\"2025-11-08T14:36:51.267985Z\",\"iopub.status.idle\":\"2025-11-08T14:37:06.368065Z\",\"shell.execute_reply.started\":\"2025-11-08T14:36:51.267945Z\",\"shell.execute_reply\":\"2025-11-08T14:37:06.367084Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Options\ndrop_ties = True     # set False to include ties\nsample    = 8000   # None to use all rows\n\n# 1) Build text and labels\ntext = (\n    \"[PROMPT] \" + df_train[\"prompt\"].fillna(\"\").astype(str) +\n    \" [ANS_A] \" + df_train[\"response_a\"].fillna(\"\").astype(str) +\n    \" [ANS_B] \" + df_train[\"response_b\"].fillna(\"\").astype(str)\n)\ny = df_train[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].values.argmax(1)\n\n# 2) Filter and subsample\nif drop_ties:\n    m = (y != 2)\n    text, y = text[m], y[m]\nif sample and len(text) > sample:\n    idx = np.random.default_rng(0).choice(len(text), size=sample, replace=False)\n    text, y = text.iloc[idx], y[idx]\n\n# 3) TF-IDF → 2D SVD\nvec = TfidfVectorizer(max_features=20000, ngram_range=(1,2),\n                      stop_words=\"english\", min_df=2, sublinear_tf=True)\nX = vec.fit_transform(text)\nZ = TruncatedSVD(n_components=2, random_state=0).fit_transform(X)\n\n# 4) Scatter plot\ncolors = {0:\"#e74c3c\", 1:\"#3498db\", 2:\"#95a5a6\"}  # A, B, Tie\nlabels = {0:\"A\", 1:\"B\", 2:\"Tie\"}\n\nplt.figure(figsize=(7,6))\nfor cls in np.unique(y):\n    m = (y == cls)\n    plt.scatter(Z[m,0], Z[m,1], s=10, alpha=0.35,\n                c=colors[cls], label=labels[cls], edgecolors=\"none\")\n\n# Optional linear decision line for A vs B\nif drop_ties:\n    lr2d = LogisticRegression(max_iter=2000).fit(Z, y)\n    x_min, x_max = Z[:,0].min()-0.5, Z[:,0].max()+0.5\n    y_min, y_max = Z[:,1].min()-0.5, Z[:,1].max()+0.5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                         np.linspace(y_min, y_max, 200))\n    zz = lr2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n    plt.contour(xx, yy, zz, levels=[0.5], colors=\"k\", linewidths=1.5)\n\nplt.title(\"A vs B separability (TF-IDF → SVD 2D)\" if drop_ties else \"A/B/Tie separability\")\nplt.xlabel(\"SVD component 1\")\nplt.ylabel(\"SVD component 2\")\nplt.legend(markerscale=2, frameon=True)\nplt.tight_layout()\nplt.show()\n\n#Clear gap between red (A) and blue (B) with a mostly straight border → a linear model (logistic regression) on your features should work.\n#Strong mixing/overlap → features aren’t expressive enough; try better text features (embeddings, more n‑grams, char n‑grams) and swap augmentation.\n#If the border would need to bend around blobs → consider nonlinear models (GBM/trees) or richer features.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:37:06.369143Z\",\"iopub.execute_input\":\"2025-11-08T14:37:06.369502Z\",\"iopub.status.idle\":\"2025-11-08T14:37:06.378748Z\",\"shell.execute_reply.started\":\"2025-11-08T14:37:06.369455Z\",\"shell.execute_reply\":\"2025-11-08T14:37:06.377869Z\"},\"jupyter\":{\"outputs_hidden\":false}}\np = df_train[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].mean()\nratio = p.max() / p.min()\nprint(f\"max/min ratio = {ratio:.3f}\")\nprint(\"is_balanced(<1.5):\", ratio < 1.5)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:37:06.379760Z\",\"iopub.execute_input\":\"2025-11-08T14:37:06.380310Z\",\"iopub.status.idle\":\"2025-11-08T14:37:06.425913Z\",\"shell.execute_reply.started\":\"2025-11-08T14:37:06.380281Z\",\"shell.execute_reply\":\"2025-11-08T14:37:06.425058Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Count model_a\nwin_a = df_train.groupby(\"model_a\")[\"winner_model_a\"].mean()\n\n# Count model_b\nwin_b = df_train.groupby(\"model_b\")[\"winner_model_b\"].mean()\n\n# Concat\nmodel_winrate = pd.concat([win_a, win_b], axis=1).fillna(0)\nmodel_winrate.columns = [\"win_as_A\", \"win_as_B\"]\n\n# total winrate\nmodel_winrate[\"total_winrate\"] = (model_winrate[\"win_as_A\"] + model_winrate[\"win_as_B\"]) / 2\n\n#check position bias\nmodel_winrate[\"bias\"]= abs(model_winrate[\"win_as_A\"] - model_winrate[\"win_as_B\"])\nmodel_winrate.sort_values(\"total_winrate\", ascending=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:37:06.426738Z\",\"iopub.execute_input\":\"2025-11-08T14:37:06.426966Z\",\"iopub.status.idle\":\"2025-11-08T14:37:06.707256Z\",\"shell.execute_reply.started\":\"2025-11-08T14:37:06.426949Z\",\"shell.execute_reply\":\"2025-11-08T14:37:06.706214Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Preprocess text\ndf_train[\"text\"] = (\"[PROMPT] \" + df_train[\"prompt\"].fillna(\"\").astype(str) +\" [ans_A] \" + df_train[\"response_a\"].fillna(\"\").astype(str) + \" [ans_B] \" + df_train[\"response_b\"].fillna(\"\").astype(str))\n\ndf_test[\"text\"] = (\"[PROMPT] \" + df_test[\"prompt\"].fillna(\"\").astype(str) +\" [ans_A] \" + df_test[\"response_a\"].fillna(\"\").astype(str) +\" [ans_B] \" + df_test[\"response_b\"].fillna(\"\").astype(str))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:37:06.710128Z\",\"iopub.execute_input\":\"2025-11-08T14:37:06.710398Z\",\"iopub.status.idle\":\"2025-11-08T14:37:06.717378Z\",\"shell.execute_reply.started\":\"2025-11-08T14:37:06.710378Z\",\"shell.execute_reply\":\"2025-11-08T14:37:06.716427Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Targets needs aslo to be in 1d\ny = np.argmax(df_train[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].values, axis=1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:37:06.718353Z\",\"iopub.execute_input\":\"2025-11-08T14:37:06.718675Z\",\"iopub.status.idle\":\"2025-11-08T14:37:07.878259Z\",\"shell.execute_reply.started\":\"2025-11-08T14:37:06.718643Z\",\"shell.execute_reply\":\"2025-11-08T14:37:07.877219Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# original text\norig_text = df_train[\"text\"]\n\n# swapped text (A<->B)\nswap_text = (\n    \"[PROMPT] \" + df_train[\"prompt\"].fillna(\"\").astype(str) +\n    \" [ans_A] \"     + df_train[\"response_b\"].fillna(\"\").astype(str) +\n    \" [ans_B] \"     + df_train[\"response_a\"].fillna(\"\").astype(str)\n)\n\n# swapped labels: A<->B, Tie stays\ny_swap = np.where(y==0, 1, np.where(y==1, 0, 2))\n\n# augment\ntext_aug = pd.concat([orig_text, swap_text], ignore_index=True)\ny_aug = np.concatenate([y, y_swap])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:37:07.879642Z\",\"iopub.execute_input\":\"2025-11-08T14:37:07.879971Z\",\"iopub.status.idle\":\"2025-11-08T14:37:08.087203Z\",\"shell.execute_reply.started\":\"2025-11-08T14:37:07.879945Z\",\"shell.execute_reply\":\"2025-11-08T14:37:08.086284Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Vectorizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nX = df_train[[\"prompt\",\"response_a\",\"response_b\"]].fillna(\"\")\n\nvec = ColumnTransformer(\n    transformers=[\n        # Prompt: words 1–2 help topic/context\n        (\"p_word\", TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.9,\n                                   sublinear_tf=True, max_features=150_000,\n                                   dtype=np.float32), \"prompt\"),\n        # Responses: character n-grams catch phrasing, punctuation, and small errors\n        (\"a_char\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5),\n                                   min_df=2, max_features=200_000,\n                                   dtype=np.float32), \"response_a\"),\n        (\"b_char\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5),\n                                   min_df=2, max_features=200_000,\n                                   dtype=np.float32), \"response_b\"),\n    ],\n    sparse_threshold=1.0\n)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-11-08T14:37:08.088280Z\",\"iopub.execute_input\":\"2025-11-08T14:37:08.088545Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nimport numpy as np\n\ny = df_train[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].values.argmax(1)\ntext = (\n    \"[PROMPT] \" + df_train[\"prompt\"].fillna(\"\").astype(str) +\n    \" [ANS_A] \" + df_train[\"response_a\"].fillna(\"\").astype(str) +\n    \" [ANS_B] \" + df_train[\"response_b\"].fillna(\"\").astype(str)\n)\n\n#this value is the trh\nuniform = np.full((len(y), 3), 1/3, dtype=float)\nprint(f\"Baseline loss: {log_loss(y, uniform):.6f}\")\n# Quick settings for speed\n\nclf = LogisticRegression(\n    solver=\"saga\",           # handles large sparse matrices\n    multi_class=\"multinomial\",\n    C=4.0,                   # inverse regularization; tune this\n    max_iter=4000\n)\n\npipe = Pipeline([(\"vec\", vec), (\"clf\", clf)])\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(pipe, X, y, cv=skf, scoring=\"neg_log_loss\", n_jobs=-1)\nprint(f\"5-fold CV log_loss: {-scores.mean():.4f} ± {scores.std():.4f}\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"clf__C\": [0.5, 1, 2, 4, 8],\n    \"vec__p_word__ngram_range\": [(1,1), (1,2)],\n    \"vec__a_char__ngram_range\": [(3,5), (4,6)],\n    \"vec__b_char__ngram_range\": [(3,5), (4,6)],\n    # Optional caps for memory/time:\n    \"vec__p_word__max_features\": [100_000, 150_000],\n    \"vec__a_char__max_features\": [150_000, 200_000],\n    \"vec__b_char__max_features\": [150_000, 200_000],\n}\n\ngrid = GridSearchCV(\n    pipe, param_grid=param_grid, scoring=\"neg_log_loss\",\n    cv=skf, n_jobs=-1, verbose=1\n)\ngrid.fit(X, y)\nprint(\"Best log_loss:\", -grid.best_score_)\nprint(\"Best params:\", grid.best_params_)\nbest_model = grid.best_estimator_\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import log_loss\nimport numpy as np\nimport pandas as pd\n\nprobs = cross_val_predict(pipe, X, y, cv=skf, method=\"predict_proba\", n_jobs=-1)\nprint(\"OOF log_loss:\", log_loss(y, probs))\n\noof = df_train.copy()\noof[\"y\"] = y\noof[[\"pA\",\"pB\",\"pTie\"]] = probs\noof[\"loss\"] = -np.log(np.take_along_axis(probs, y[:,None], axis=1)).ravel()\noof.sort_values(\"loss\", ascending=False).head(15)[\n    [\"prompt\",\"response_a\",\"response_b\",\"y\",\"pA\",\"pB\",\"pTie\",\"loss\"]\n]\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy import sparse\n\ndef simple_feats(df):\n    import numpy as np\n    pa = df[\"response_a\"].fillna(\"\")\n    pb = df[\"response_b\"].fillna(\"\")\n    pr = df[\"prompt\"].fillna(\"\")\n    def ov(a, b): return np.array([len(set(a.split()) & set(b.split()))], dtype=float)\n    feats = []\n    for a, b, p in zip(pa, pb, pr):\n        feats.append([\n            len(a), len(b),\n            a.count(\"?\"), b.count(\"?\"),\n            a.count(\"!\"), b.count(\"!\"),\n            len(set(a.split())), len(set(b.split())),\n            len(set(p.split()) & set(a.split())),\n            len(set(p.split()) & set(b.split())),\n        ])\n    return np.array(feats, dtype=float)\n\nnum_feats = FunctionTransformer(simple_feats, validate=False)\n\ncombined = ColumnTransformer(\n    transformers=list(vec.transformers) + [\n        (\"num\", num_feats, [\"prompt\",\"response_a\",\"response_b\"])\n    ],\n    sparse_threshold=1.0\n)\n\npipe2 = Pipeline([(\"vec\", combined), (\"clf\", clf)])\nscores = cross_val_score(pipe2, X, y, cv=skf, scoring=\"neg_log_loss\", n_jobs=-1)\nprint(f\"With simple features, log_loss: {-scores.mean():.4f}\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ndef make_pairwise(df):\n    # yA: 1 if A wins; 0 otherwise\n    yA = (df[\"winner_model_a\"] == 1).astype(int).values\n    yB = (df[\"winner_model_b\"] == 1).astype(int).values\n\n    XA = df[[\"prompt\",\"response_a\"]].rename(columns={\"response_a\":\"response\"})\n    XB = df[[\"prompt\",\"response_b\"]].rename(columns={\"response_b\":\"response\"})\n    X_pair = pd.concat([XA, XB], ignore_index=True)\n    y_pair = np.concatenate([yA, yB])\n    return X_pair, y_pair\n\nX_pair, y_pair = make_pairwise(df_train)\n\nvec_pw = ColumnTransformer([\n    (\"p_word\", TfidfVectorizer(ngram_range=(1,2), min_df=2, sublinear_tf=True), \"prompt\"),\n    (\"r_char\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=2), \"response\"),\n])\n\nclf_pw = LogisticRegression(solver=\"saga\", max_iter=4000)\npipe_pw = Pipeline([(\"vec\", vec_pw), (\"clf\", clf_pw)])\n\nscores = cross_val_score(pipe_pw, X_pair, y_pair, cv=skf, scoring=\"neg_log_loss\", n_jobs=-1)\nprint(\"Pairwise OOF log_loss (binary):\", -scores.mean())\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import VotingClassifier\n\nsgd = Pipeline([\n    (\"vec\", vec),\n    (\"clf\", SGDClassifier(loss=\"log_loss\", alpha=1e-4, max_iter=2000, random_state=42))\n])\n\nlr = pipe  # from above\n\n# Calibrate SGD (often underconfident/overconfident)\nsgd_cal = CalibratedClassifierCV(sgd, method=\"isotonic\", cv=3)\n\nvoter = VotingClassifier(\n    estimators=[(\"lr\", lr), (\"sgd\", sgd_cal)],\n    voting=\"soft\", weights=[1,1]\n)\n\nscores = cross_val_score(voter, X, y, cv=skf, scoring=\"neg_log_loss\", n_jobs=-1)\nprint(\"Ensemble log_loss:\", -scores.mean())\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n# Models\nlog_reg = LogisticRegression(max_iter=2000, class_weight=\"balanced\", C=2.0)\nrf = RandomForestClassifier(n_estimators=300, max_depth=20, n_jobs=-1, random_state=42)\n\nclf = VotingClassifier(estimators=[(\"lr\", log_reg), (\"rf\", rf)],voting=\"soft\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# Train\nX = vectorizer.fit_transform(text_aug)\nclf.fit(X, y_aug)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nprint(X.shape)                         # (n_rows, n_features) e.g., (55000, 5000)\nprint(len(vectorizer.get_feature_names_out()))\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# Predict on test\nX_test = vectorizer.transform(df_test[\"text\"])\nprobs = clf.predict_proba(X_test)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# Submission\nsubmission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"winner_model_a\": probs[:,0],\n    \"winner_model_b\": probs[:,1],\n    \"winner_tie\": probs[:,2],\n})\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nfrom IPython.display import display\ndisplay(submission.head())\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created successfully: submission.csv\")","metadata":{"_uuid":"1ae0d0ef-0722-4ab6-8081-ec737873a41b","_cell_guid":"8f424237-bc83-497b-a3b3-45e1788927ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}